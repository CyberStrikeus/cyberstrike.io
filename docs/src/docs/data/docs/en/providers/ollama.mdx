---
title: Ollama (Local)
description: Run local AI models with Ollama for Cyberstrike
sidebar:
  order: 6
  label: Ollama
---

Ollama enables running AI models locally on your machine for private, offline security testing.

{/* TODO: Screenshot - Ollama model selection */}
<div className="border-2 border-dashed border-gray-400 dark:border-gray-600 rounded-lg p-8 my-6 text-center bg-gray-100 dark:bg-gray-800">
  <p className="text-gray-500 dark:text-gray-400 font-mono text-sm">ðŸ“¸ SCREENSHOT: ollama-model-select.png</p>
  <p className="text-gray-400 dark:text-gray-500 text-xs mt-2">Ollama local model selection</p>
</div>

## Overview

Ollama advantages:

- Complete data privacy
- No API costs
- Offline operation
- No rate limits
- Full control over models

## Installation

### macOS

```bash
brew install ollama
```

### Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows

Download from [ollama.com/download](https://ollama.com/download)

## Starting Ollama

### Start Server

```bash
ollama serve
```

The server runs on `http://localhost:11434` by default.

### Verify Installation

```bash
ollama --version
```

## Available Models

### Recommended Models

| Model | Size | Best For |
|-------|------|----------|
| llama3.3:70b | 40GB | Best quality, requires high-end GPU |
| llama3.3 | 4.7GB | Good balance of quality and speed |
| codellama:34b | 19GB | Code analysis |
| mistral | 4.1GB | Fast general purpose |
| mixtral:8x7b | 26GB | High quality, efficient |
| deepseek-coder:33b | 19GB | Code security review |
| qwen2.5:72b | 41GB | Excellent reasoning |

### Pull Models

```bash
# Recommended for security testing
ollama pull llama3.3

# For code review
ollama pull codellama:34b

# Lightweight option
ollama pull mistral
```

### List Installed Models

```bash
ollama list
```

## Configuration

### Basic Setup

```json title="~/.cyberstrike/config.json"
{
  "provider": {
    "ollama": {
      "options": {
        "baseURL": "http://localhost:11434"
      }
    }
  },
  "model": "ollama/llama3.3"
}
```

### Custom Host

For remote Ollama server:

```json
{
  "provider": {
    "ollama": {
      "options": {
        "baseURL": "http://192.168.1.100:11434"
      }
    }
  }
}
```

## Usage

### Command Line

```bash
cyberstrike --model ollama/llama3.3
```

### In-Session

```
/model
# Select Ollama model
```

## Model Configuration

### Context Length

Increase context window:

```bash
ollama run llama3.3 --num-ctx 8192
```

Or in Modelfile:

```
FROM llama3.3
PARAMETER num_ctx 8192
```

### GPU Layers

Control GPU usage:

```bash
OLLAMA_NUM_GPU=999 ollama serve  # Use all GPU layers
OLLAMA_NUM_GPU=0 ollama serve    # CPU only
```

### Memory Management

```bash
# Limit VRAM usage
OLLAMA_MAX_VRAM=8G ollama serve
```

## Custom Models

### Create Modelfile

```dockerfile title="Modelfile"
FROM llama3.3

SYSTEM """
You are a security testing assistant specialized in:
- Web application vulnerabilities
- Code security review
- Network penetration testing

Always follow OWASP guidelines and report findings with:
- Vulnerability name
- Severity (Critical/High/Medium/Low)
- Evidence
- Remediation steps
"""

PARAMETER temperature 0.7
PARAMETER num_ctx 8192
```

### Build Custom Model

```bash
ollama create security-assistant -f Modelfile
```

### Use Custom Model

```bash
cyberstrike --model ollama/security-assistant
```

## Performance Optimization

### Hardware Requirements

| Model Size | Min RAM | Recommended GPU |
|------------|---------|-----------------|
| 7B | 8GB | 8GB VRAM |
| 13B | 16GB | 12GB VRAM |
| 34B | 32GB | 24GB VRAM |
| 70B | 64GB | 48GB VRAM |

### Quantization

Use quantized models for lower memory:

```bash
ollama pull llama3.3:q4_0  # 4-bit quantization
ollama pull llama3.3:q8_0  # 8-bit quantization
```

### Parallel Requests

Enable concurrent requests:

```bash
OLLAMA_NUM_PARALLEL=4 ollama serve
```

## Offline Usage

### Pre-download Models

```bash
ollama pull llama3.3
ollama pull codellama
```

### Air-Gapped Systems

1. Download models on connected system
2. Copy `~/.ollama/models/` to air-gapped system
3. Run Ollama without internet

## Docker Deployment

### Run in Container

```bash
docker run -d \
  --gpus all \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama
```

### Pull Models in Container

```bash
docker exec -it ollama ollama pull llama3.3
```

## API Compatibility

Ollama supports OpenAI-compatible API:

```json
{
  "provider": {
    "openai": {
      "options": {
        "baseURL": "http://localhost:11434/v1",
        "apiKey": "ollama"
      }
    }
  },
  "model": "openai/llama3.3"
}
```

## Troubleshooting

### Connection Refused

```
Error: Connection refused
```

Ensure Ollama is running:

```bash
ollama serve
```

### Out of Memory

```
Error: CUDA out of memory
```

Solutions:
- Use smaller model
- Use quantized version
- Reduce context length
- Set `OLLAMA_NUM_GPU=0` for CPU

### Slow Performance

- Enable GPU acceleration
- Use quantized models
- Increase `num_parallel`
- Check thermal throttling

<Aside variant="tip">
  For best results, use llama3.3:70b with a high-end GPU. For resource-limited systems, mistral provides good quality with lower requirements.
</Aside>

## Related Documentation

- [Providers Overview](/docs/providers) - All providers
- [Custom Providers](/docs/providers/custom-providers) - OpenAI-compatible setup
- [Configuration](/docs/configuration) - Full options
